#!/usr/bin/env python3
"""
LIHTC Fine-Tuned LLM Deployment System
Comprehensive deployment pipeline for LIHTC-specialized language model

Created: 2025-08-01
Agent: Strike Leader
Mission: Deploy industry's first LIHTC fine-tuned LLM with RAG integration
"""

import os
import json
import logging
from datetime import datetime
from pathlib import Path
import subprocess
import sys
from typing import Dict, List, Optional, Any
import torch
from dataclasses import dataclass

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('lihtc_llm_deployment.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class DeploymentConfig:
    """Configuration for LIHTC LLM deployment"""
    base_model: str = "meta-llama/Llama-3.1-8B-Instruct"
    output_dir: str = "./lihtc_finetuned_model"
    training_data_path: str = "./lihtc_training_data_alpaca.json"
    
    # LoRA Configuration
    lora_rank: int = 16
    lora_alpha: int = 32
    lora_dropout: float = 0.1
    target_modules: List[str] = None
    
    # Training Configuration
    num_train_epochs: int = 3
    learning_rate: float = 2e-5
    per_device_train_batch_size: int = 4
    gradient_accumulation_steps: int = 4
    warmup_steps: int = 100
    max_steps: int = -1
    
    # System Requirements
    fp16: bool = True
    gradient_checkpointing: bool = True
    dataloader_num_workers: int = 4
    
    def __post_init__(self):
        if self.target_modules is None:
            self.target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]

class LIHTCLLMDeployment:
    """Deploys fine-tuned LIHTC LLM with comprehensive training and validation"""
    
    def __init__(self, config: DeploymentConfig):
        self.config = config
        self.deployment_dir = Path("./lihtc_llm_deployment")
        self.deployment_dir.mkdir(exist_ok=True)
        
        logger.info("LIHTC LLM Deployment System Initialized")
        logger.info(f"Base Model: {config.base_model}")
        logger.info(f"Training Data: {config.training_data_path}")
    
    def check_system_requirements(self) -> Dict[str, Any]:
        """Check system requirements for fine-tuning"""
        logger.info("Checking system requirements...")
        
        requirements = {
            "gpu_available": torch.cuda.is_available(),
            "gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0,
            "gpu_memory": [],
            "python_version": sys.version,
            "torch_version": torch.__version__
        }
        
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                gpu_props = torch.cuda.get_device_properties(i)
                memory_gb = gpu_props.total_memory / (1024**3)
                requirements["gpu_memory"].append({
                    "device": i,
                    "name": gpu_props.name,
                    "memory_gb": round(memory_gb, 2)
                })
        
        logger.info(f"System Requirements: {json.dumps(requirements, indent=2)}")
        return requirements
    
    def install_dependencies(self) -> bool:
        """Install required dependencies for fine-tuning"""
        logger.info("Installing fine-tuning dependencies...")
        
        dependencies = [
            "transformers>=4.36.0",
            "peft>=0.7.0",
            "accelerate>=0.25.0",
            "bitsandbytes>=0.41.0",
            "datasets>=2.16.0",
            "torch>=2.1.0",
            "trl>=0.7.0"
        ]
        
        try:
            for dep in dependencies:
                logger.info(f"Installing {dep}...")
                subprocess.run([sys.executable, "-m", "pip", "install", dep], 
                             check=True, capture_output=True)
            
            logger.info("‚úÖ All dependencies installed successfully")
            return True
            
        except subprocess.CalledProcessError as e:
            logger.error(f"‚ùå Failed to install dependencies: {e}")
            return False
    
    def prepare_training_script(self) -> str:
        """Generate fine-tuning training script"""
        logger.info("Preparing training script...")
        
        script_content = f'''#!/usr/bin/env python3
"""
LIHTC Fine-Tuning Script
Generated: {datetime.now().isoformat()}
"""

import os
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import load_dataset
import json

def main():
    # Configuration
    model_name = "{self.config.base_model}"
    output_dir = "{self.config.output_dir}"
    training_data = "{self.config.training_data_path}"
    
    print("üöÄ Starting LIHTC LLM Fine-Tuning...")
    print(f"Base Model: {{model_name}}")
    print(f"Training Data: {{training_data}}")
    
    # Load tokenizer and model
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        device_map="auto"
    )
    
    # LoRA Configuration
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r={self.config.lora_rank},
        lora_alpha={self.config.lora_alpha},
        lora_dropout={self.config.lora_dropout},
        target_modules={self.config.target_modules}
    )
    
    # Apply LoRA
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    # Load and prepare dataset
    def format_instruction(example):
        if "instruction" in example and "response" in example:
            text = f"### Instruction:\\n{{example['instruction']}}\\n\\n### Response:\\n{{example['response']}}"
        elif "text" in example:
            text = example["text"]
        else:
            text = str(example)
        return {{"text": text}}
    
    # Load training data
    dataset = load_dataset("json", data_files=training_data, split="train")
    dataset = dataset.map(format_instruction)
    
    def tokenize_function(examples):
        return tokenizer(examples["text"], truncation=True, padding=False, max_length=2048)
    
    tokenized_dataset = dataset.map(tokenize_function, batched=True)
    
    # Data collator
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False
    )
    
    # Training arguments
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs={self.config.num_train_epochs},
        per_device_train_batch_size={self.config.per_device_train_batch_size},
        gradient_accumulation_steps={self.config.gradient_accumulation_steps},
        learning_rate={self.config.learning_rate},
        warmup_steps={self.config.warmup_steps},
        fp16={str(self.config.fp16).lower()},
        gradient_checkpointing={str(self.config.gradient_checkpointing).lower()},
        dataloader_num_workers={self.config.dataloader_num_workers},
        save_strategy="epoch",
        evaluation_strategy="no",
        logging_steps=10,
        logging_dir=f"{{output_dir}}/logs",
        report_to="none"
    )
    
    # Initialize trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator,
        tokenizer=tokenizer
    )
    
    # Start training
    print("üî• Starting fine-tuning process...")
    trainer.train()
    
    # Save the model
    trainer.save_model()
    tokenizer.save_pretrained(output_dir)
    
    print(f"‚úÖ Fine-tuning complete! Model saved to {{output_dir}}")

if __name__ == "__main__":
    main()
'''
        
        script_path = self.deployment_dir / "train_lihtc_llm.py"
        script_path.write_text(script_content)
        
        logger.info(f"‚úÖ Training script created: {script_path}")
        return str(script_path)
    
    def create_inference_interface(self) -> str:
        """Create inference interface for the fine-tuned model"""
        logger.info("Creating inference interface...")
        
        interface_content = '''#!/usr/bin/env python3
"""
LIHTC Fine-Tuned LLM Inference Interface
Professional interface for LIHTC Q&A with RAG integration
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import json
import chromadb
from pathlib import Path
from typing import List, Dict, Optional

class LIHTCInferenceEngine:
    """Professional LIHTC Q&A system with fine-tuned LLM + RAG"""
    
    def __init__(self, model_path: str, chromadb_path: str = "./lihtc_definitions_chromadb"):
        self.model_path = model_path
        self.chromadb_path = chromadb_path
        
        # Load fine-tuned model
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.tokenizer.pad_token = self.tokenizer.eos_token
        
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto"
        )
        
        # Load ChromaDB for RAG
        self.chroma_client = chromadb.PersistentClient(path=chromadb_path)
        self.collection = self.chroma_client.get_collection("lihtc_definitions")
        
        print("‚úÖ LIHTC Inference Engine Ready")
        print(f"Model: {model_path}")
        print(f"ChromaDB: {chromadb_path}")
    
    def retrieve_context(self, query: str, n_results: int = 5) -> List[Dict]:
        """Retrieve relevant context from RAG system"""
        results = self.collection.query(
            query_texts=[query],
            n_results=n_results,
            include=["documents", "metadatas"]
        )
        
        context_items = []
        for i, doc in enumerate(results["documents"][0]):
            metadata = results["metadatas"][0][i]
            context_items.append({
                "content": doc,
                "jurisdiction": metadata.get("jurisdiction", "Unknown"),
                "source": metadata.get("source", "QAP"),
                "relevance_score": 1.0 - (i / n_results)  # Simple relevance scoring
            })
        
        return context_items
    
    def generate_response(self, query: str, context: Optional[List[Dict]] = None, max_length: int = 1024) -> str:
        """Generate response using fine-tuned model with context"""
        
        # Prepare context if provided
        context_text = ""
        if context:
            context_text = "\\n\\nRelevant LIHTC Information:\\n"
            for item in context[:3]:  # Use top 3 results
                context_text += f"- {item['jurisdiction']}: {item['content'][:200]}...\\n"
        
        # Format prompt
        prompt = f"""### Instruction:
Answer this LIHTC (Low-Income Housing Tax Credit) question based on your specialized knowledge and the provided context. Be specific, accurate, and cite relevant jurisdictions when applicable.

Question: {query}{context_text}

### Response:"""
        
        # Tokenize and generate
        inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1800)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_length,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = response.split("### Response:")[-1].strip()
        
        return response
    
    def lihtc_qa(self, question: str) -> Dict:
        """Complete LIHTC Q&A with RAG integration"""
        print(f"üîç Processing: {question}")
        
        # Retrieve context
        context = self.retrieve_context(question)
        
        # Generate response
        response = self.generate_response(question, context)
        
        return {
            "question": question,
            "response": response,
            "context_sources": [
                {"jurisdiction": item["jurisdiction"], "relevance": item["relevance_score"]}
                for item in context[:3]
            ],
            "context_count": len(context)
        }

def main():
    """Interactive LIHTC Q&A session"""
    engine = LIHTCInferenceEngine("./lihtc_finetuned_model")
    
    print("üè¢ LIHTC Expert Assistant Ready!")
    print("Ask questions about Low-Income Housing Tax Credits, QAPs, and compliance requirements.")
    print("Type 'quit' to exit.\\n")
    
    while True:
        question = input("‚ùì Your LIHTC Question: ").strip()
        
        if question.lower() in ['quit', 'exit', 'q']:
            print("üëã Goodbye!")
            break
        
        if not question:
            continue
        
        try:
            result = engine.lihtc_qa(question)
            print(f"\\nüí° Response:")
            print(result["response"])
            print(f"\\nüìö Sources: {len(result['context_sources'])} jurisdictions referenced")
            print("-" * 80 + "\\n")
            
        except Exception as e:
            print(f"‚ùå Error: {e}\\n")

if __name__ == "__main__":
    main()
'''
        
        interface_path = self.deployment_dir / "lihtc_inference.py"
        interface_path.write_text(interface_content)
        
        logger.info(f"‚úÖ Inference interface created: {interface_path}")
        return str(interface_path)
    
    def deploy_system(self) -> Dict[str, Any]:
        """Deploy complete LIHTC fine-tuned LLM system"""
        logger.info("üöÄ Starting LIHTC LLM System Deployment...")
        
        deployment_status = {
            "start_time": datetime.now().isoformat(),
            "system_requirements": {},
            "dependencies_installed": False,
            "training_script_created": False,
            "inference_interface_created": False,
            "deployment_ready": False
        }
        
        try:
            # Check system requirements
            deployment_status["system_requirements"] = self.check_system_requirements()
            
            # Install dependencies  
            deployment_status["dependencies_installed"] = self.install_dependencies()
            
            # Create training script
            training_script = self.prepare_training_script()
            deployment_status["training_script_created"] = bool(training_script)
            deployment_status["training_script_path"] = training_script
            
            # Create inference interface
            inference_interface = self.create_inference_interface()
            deployment_status["inference_interface_created"] = bool(inference_interface)
            deployment_status["inference_interface_path"] = inference_interface
            
            # Create deployment instructions
            instructions = self.create_deployment_instructions()
            deployment_status["instructions_path"] = instructions
            
            deployment_status["deployment_ready"] = (
                deployment_status["dependencies_installed"] and
                deployment_status["training_script_created"] and
                deployment_status["inference_interface_created"]
            )
            
            deployment_status["completion_time"] = datetime.now().isoformat()
            
            if deployment_status["deployment_ready"]:
                logger.info("‚úÖ LIHTC LLM System Deployment Complete!")
                logger.info("Ready for fine-tuning execution!")
            else:
                logger.error("‚ùå Deployment incomplete - check individual components")
            
        except Exception as e:
            logger.error(f"‚ùå Deployment failed: {e}")
            deployment_status["error"] = str(e)
        
        # Save deployment status
        status_path = self.deployment_dir / "deployment_status.json"
        with open(status_path, 'w') as f:
            json.dump(deployment_status, f, indent=2)
        
        return deployment_status
    
    def create_deployment_instructions(self) -> str:
        """Create comprehensive deployment instructions"""
        instructions_content = f'''# LIHTC Fine-Tuned LLM Deployment Instructions

Generated: {datetime.now().isoformat()}

## üéØ DEPLOYMENT OVERVIEW

This system deploys the industry's first LIHTC-specialized fine-tuned language model with RAG integration.

**Base Model**: {self.config.base_model}
**Training Data**: 3,027 LIHTC-specific examples
**Fine-Tuning Method**: LoRA (Low-Rank Adaptation)

## üöÄ STEP-BY-STEP DEPLOYMENT

### Step 1: System Requirements Check
- GPU: NVIDIA GPU with 16GB+ VRAM (RTX 4090 or A100 recommended)
- RAM: 32GB+ system memory
- Storage: 100GB+ free space
- Python: 3.8+ with CUDA support

### Step 2: Execute Fine-Tuning
```bash
cd {self.deployment_dir}
python3 train_lihtc_llm.py
```

Expected training time: 2-4 hours on RTX 4090

### Step 3: Test Inference
```bash
python3 lihtc_inference.py
```

### Step 4: Production Deployment
- Set up FastAPI web interface
- Configure load balancing
- Deploy ChromaDB for RAG
- Set up monitoring and logging

## üìä TRAINING CONFIGURATION

- **LoRA Rank**: {self.config.lora_rank}
- **LoRA Alpha**: {self.config.lora_alpha}
- **Learning Rate**: {self.config.learning_rate}
- **Epochs**: {self.config.num_train_epochs}
- **Batch Size**: {self.config.per_device_train_batch_size}

## üéØ EXPECTED RESULTS

After fine-tuning, the model will:
- Understand LIHTC terminology and regulations
- Provide jurisdiction-specific responses
- Integrate seamlessly with RAG system
- Generate accurate, professional answers

## üèÜ BUSINESS VALUE

- **Industry First**: Only LIHTC-specialized fine-tuned LLM
- **54 Jurisdictions**: Complete US coverage
- **Professional Quality**: Ready for commercial deployment
- **Revenue Ready**: SaaS platform foundation

## üîß TROUBLESHOOTING

### Common Issues:
1. **CUDA Out of Memory**: Reduce batch size to 2 or 1
2. **Model Loading Error**: Ensure transformers>=4.36.0
3. **Training Slow**: Enable gradient checkpointing

### Support:
- Check logs in ./lihtc_llm_deployment/
- Review deployment_status.json for details
- Monitor GPU usage during training

---

**üèÜ LIHTC AI REVOLUTION READY FOR DEPLOYMENT!**
'''
        
        instructions_path = self.deployment_dir / "DEPLOYMENT_INSTRUCTIONS.md"
        instructions_path.write_text(instructions_content)
        
        logger.info(f"‚úÖ Deployment instructions created: {instructions_path}")
        return str(instructions_path)

def main():
    """Main deployment execution"""
    print("‚öîÔ∏è STRIKE LEADER: LIHTC LLM DEPLOYMENT INITIATED")
    print("=" * 60)
    
    # Initialize deployment configuration
    config = DeploymentConfig()
    
    # Create deployment system
    deployment = LIHTCLLMDeployment(config)
    
    # Execute deployment
    status = deployment.deploy_system()
    
    print("=" * 60)
    if status.get("deployment_ready"):
        print("‚úÖ DEPLOYMENT COMPLETE - READY FOR FINE-TUNING!")
        print(f"üìÅ Deployment Directory: {deployment.deployment_dir}")
        print("üöÄ Execute: python3 train_lihtc_llm.py")
    else:
        print("‚ùå DEPLOYMENT INCOMPLETE - CHECK LOGS")
    
    return status

if __name__ == "__main__":
    main()