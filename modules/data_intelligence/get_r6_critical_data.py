#!/usr/bin/env python3
"""
Download R6 South Central data - CRITICAL for Texas LIHTC
The crawler completely missed R6's extensive subdirectories
R6 covers: TX, LA, OK, AR, NM
"""

import requests
import json
from pathlib import Path
from datetime import datetime
import time

class R6CriticalDownloader:
    def __init__(self):
        self.base_path = Path("/Users/williamrice/Library/CloudStorage/Dropbox-HERR/Bill Rice/Colosseum")
        self.output_path = self.base_path / "data_sets" / "federal" / "EPA_Data" / "Regional" / "R6"
        self.base_url = "https://edg.epa.gov/data/public/R6/"
        self.downloaded = []
        self.start_time = datetime.now()
        
    def download_file(self, subdir, filename, description, skip_large=True):
        """Download a specific file from R6"""
        url = f"{self.base_url}{subdir}/{filename}" if subdir else f"{self.base_url}{filename}"
        
        # Create output directory
        if subdir:
            output_dir = self.output_path / subdir
        else:
            output_dir = self.output_path
        
        output_dir.mkdir(parents=True, exist_ok=True)
        output_file = output_dir / filename
        
        if output_file.exists():
            print(f"  ‚úì Already exists: {filename}")
            return True
        
        print(f"  Downloading: {filename}")
        print(f"    Description: {description}")
        
        try:
            # Check size first for large files
            if skip_large:
                head = requests.head(url, timeout=10)
                content_length = int(head.headers.get('content-length', 0))
                if content_length > 500 * 1024 * 1024:  # 500 MB
                    print(f"    ‚è≠Ô∏è Skipping large file: {content_length/(1024*1024):.1f} MB")
                    return False
            
            response = requests.get(url, stream=True, timeout=60)
            if response.status_code == 200:
                total_size = int(response.headers.get('content-length', 0))
                downloaded = 0
                
                with open(output_file, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=1024*1024):
                        if chunk:
                            f.write(chunk)
                            downloaded += len(chunk)
                            
                            # Progress for files >10MB
                            if total_size > 10*1024*1024 and downloaded % (10*1024*1024) < 1024*1024:
                                percent = (downloaded / total_size) * 100 if total_size > 0 else 0
                                print(f"      Progress: {percent:.1f}% ({downloaded/(1024*1024):.1f} MB)")
                
                size_mb = output_file.stat().st_size / (1024 * 1024)
                print(f"    ‚úÖ Success: {size_mb:.2f} MB")
                
                self.downloaded.append({
                    'subdir': subdir,
                    'file': filename,
                    'size_mb': size_mb,
                    'description': description
                })
                return True
            else:
                print(f"    ‚ùå HTTP {response.status_code}")
                return False
        except Exception as e:
            print(f"    ‚ùå Error: {str(e)[:100]}")
            return False
    
    def get_r6_critical_data(self):
        """Download R6 South Central environmental data"""
        
        print("\n" + "="*80)
        print("üö® CRITICAL: DOWNLOADING R6 SOUTH CENTRAL DATA")
        print("Region 6: Texas, Louisiana, Oklahoma, Arkansas, New Mexico")
        print("="*80)
        
        # Critical R6 files for LIHTC screening
        r6_priority_files = [
            # NPL Superfund sites - CRITICAL
            {'subdir': 'NPL', 'file': 'EPA_R6_NPL_SF_Points_20230331.zip', 
             'desc': '‚≠ê NPL Superfund site points (2023)'},
            {'subdir': 'NPL', 'file': 'EPA_R6_NPL_SF_Bdry_20230301.zip',
             'desc': '‚≠ê NPL Superfund boundaries (2023)'},
            
            # Brownfields - HIGH PRIORITY
            {'subdir': 'Brownfields', 'file': 'R6Brownfields_062612.zip',
             'desc': '‚≠ê R6 Brownfields sites'},
            
            # Aquifers - Important for water
            {'subdir': 'Aquifers', 'file': 'R6SSAquifers.zip',
             'desc': 'Sole Source Aquifers'},
            
            # Air quality nonattainment
            {'subdir': 'Nonattainment', 'file': 'Nonattainment_July2013.zip',
             'desc': 'Air quality nonattainment areas'},
            {'subdir': 'Nonattainment', 'file': 'MaintenanceEAC_July2013.zip',
             'desc': 'Maintenance areas'},
            
            # Metadata with site info
            {'subdir': 'metadata', 'file': 'EPA_R6_NPL_Points_2023.xml',
             'desc': 'NPL points metadata'},
            {'subdir': 'metadata', 'file': 'Region6.json',
             'desc': 'R6 region metadata'},
            
            # TEAP environmental assessment
            {'subdir': 'TEAP', 'file': 'teap_data.zip',
             'desc': 'Tribal Environmental Assessment'},
            
            # REAP assessments (smaller files)
            {'subdir': 'REAP', 'file': 'r6reap_rarity.zip',
             'desc': 'Environmental rarity assessment (14 MB)'},
            
            # Skip GISST (721 MB - too large)
        ]
        
        success_count = 0
        
        print("\nüì• Downloading priority environmental data:")
        print("-" * 40)
        
        for item in r6_priority_files:
            print(f"\n{item['subdir']}/{item['file']}:")
            if self.download_file(item['subdir'], item['file'], item['desc']):
                success_count += 1
            time.sleep(1)  # Be nice
        
        # Save summary
        summary = {
            'download_date': self.start_time.isoformat(),
            'region': 'R6 - South Central (TX, LA, OK, AR, NM)',
            'critical_importance': 'Texas LIHTC development',
            'files_attempted': len(r6_priority_files),
            'files_successful': success_count,
            'total_mb': sum(d['size_mb'] for d in self.downloaded),
            'downloads': self.downloaded,
            'note': 'Skipped GISST (721 MB) due to size'
        }
        
        summary_file = self.output_path.parent.parent.parent / "r6_critical_data_summary.json"
        with open(summary_file, 'w') as f:
            json.dump(summary, f, indent=2)
        
        duration = (datetime.now() - self.start_time).total_seconds() / 60
        
        print("\n" + "="*80)
        print("R6 CRITICAL DATA SUMMARY")
        print("="*80)
        print(f"Duration: {duration:.1f} minutes")
        print(f"Files downloaded: {success_count} of {len(r6_priority_files)}")
        print(f"Total size: {summary['total_mb']:.2f} MB")
        
        if self.downloaded:
            print("\nüéØ Critical acquisitions for Texas LIHTC:")
            for d in self.downloaded:
                if 'NPL' in d['file'] or 'Brownfield' in d['file']:
                    print(f"  ‚≠ê {d['file']}: {d['description']}")
        
        print("\nüìç R6 Coverage Impact:")
        print("  ‚úÖ Texas - Major LIHTC market now covered")
        print("  ‚úÖ Louisiana - Coastal environmental data")
        print("  ‚úÖ Oklahoma, Arkansas, New Mexico included")
        print("="*80)
        
        return success_count

def main():
    print("\nüö® R6 SOUTH CENTRAL CRITICAL DATA RECOVERY")
    print("Getting Texas and surrounding states environmental data")
    print("This is CRITICAL for Texas LIHTC development!")
    print("-"*60)
    
    downloader = R6CriticalDownloader()
    count = downloader.get_r6_critical_data()
    
    print(f"\n‚úÖ R6 CRITICAL DATA RECOVERY COMPLETE")
    print(f"Successfully downloaded {count} R6 files")
    print("\nüéØ MAJOR ACHIEVEMENT:")
    print("  - Texas NPL Superfund sites acquired")
    print("  - R6 Brownfields data obtained")
    print("  - 9 of 10 EPA regions now have data!")
    print("  - Only R2 (NY/NJ) remains truly empty")

if __name__ == "__main__":
    main()