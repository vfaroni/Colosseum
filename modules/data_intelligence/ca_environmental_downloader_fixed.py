#!/usr/bin/env python3
"""
California Environmental Data Downloader - FIXED VERSION
With proper API endpoints and README.txt generation
WINGMAN Execution - Mission CA-ENV-2025-002
Date: 2025-08-09
"""

import os
import json
import time
import logging
import requests
import pandas as pd
import geopandas as gpd
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional
from io import StringIO
import zipfile

class CaliforniaEnvironmentalDownloader:
    """Download California environmental data with proper documentation"""
    
    def __init__(self, base_path: str = None):
        """Initialize downloader"""
        self.base_path = Path(base_path or "/Users/williamrice/Library/CloudStorage/Dropbox-HERR/Bill Rice/Colosseum")
        self.data_path = self.base_path / "data_sets" / "california" / "CA_Environmental_Data"
        self.data_path.mkdir(parents=True, exist_ok=True)
        
        # Setup logging
        self.setup_logging()
        self.download_date = datetime.now().strftime('%Y-%m-%d')
        
    def setup_logging(self):
        """Configure logging"""
        log_file = self.data_path / f"download_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def create_readme(self, county_path: Path, county_name: str, datasets: Dict):
        """Create README.txt for dataset documentation"""
        readme_content = f"""DATASET: California Environmental Data - {county_name} County
SOURCE: Multiple California State Agencies
DOWNLOAD DATE: {self.download_date}
DESCRIPTION: Environmental contamination sites, cleanup locations, and regulatory data
FORMAT: CSV, GeoJSON
COVERAGE: {county_name} County, California

DATA SOURCES INCLUDED:
====================
"""
        
        for dataset_name, dataset_info in datasets.items():
            if dataset_info.get('status') == 'completed':
                readme_content += f"""
{dataset_name.upper()}:
- SOURCE: {dataset_info.get('source', 'N/A')}
- SOURCE URL: {dataset_info.get('url', 'N/A')}
- RECORDS: {dataset_info.get('records', 0)}
- FILE: {dataset_info.get('file', 'N/A')}
- SOURCE DATE: {dataset_info.get('source_date', 'Current as of download')}
"""
        
        readme_content += f"""
UPDATE FREQUENCY: Varies by source (typically monthly to quarterly)

NOTES:
- EnviroStor: Department of Toxic Substances Control cleanup sites
- GeoTracker: State Water Resources Control Board LUST and cleanup sites  
- FEMA NFHL: National Flood Hazard Layer flood zones
- Data filtered to county boundaries where applicable
- Coordinate system: WGS84 (EPSG:4326)

GENERATED BY: Colosseum LIHTC Platform - WINGMAN Agent
MISSION: CA Environmental Data Acquisition
"""
        
        readme_file = county_path / "README.txt"
        with open(readme_file, 'w') as f:
            f.write(readme_content)
        
        self.logger.info(f"Created README.txt for {county_name} County")
        
    def download_envirostor_data(self, county_name: str, county_path: Path) -> Dict:
        """Download EnviroStor data using correct API"""
        result = {
            'source': 'CA DTSC EnviroStor',
            'status': 'failed',
            'records': 0,
            'file': None,
            'url': 'https://www.envirostor.dtsc.ca.gov'
        }
        
        try:
            # Use California Open Data Portal API for EnviroStor
            url = "https://data.ca.gov/api/3/action/datastore_search"
            
            # EnviroStor dataset resource ID (verified working)
            params = {
                'resource_id': 'e0f6b8e3-72f4-4a53-bbce-e13c8f4e3b6c',
                'limit': 10000,
                'q': county_name
            }
            
            response = requests.get(url, params=params, timeout=60)
            
            if response.status_code == 200:
                data = response.json()
                
                if data.get('success') and data.get('result', {}).get('records'):
                    records = data['result']['records']
                    
                    # Convert to DataFrame
                    df = pd.DataFrame(records)
                    
                    # Filter for county if county column exists
                    if 'County' in df.columns:
                        df = df[df['County'].str.contains(county_name, case=False, na=False)]
                    
                    if len(df) > 0:
                        output_file = county_path / f"{county_name.replace(' ', '_')}_envirostor.csv"
                        df.to_csv(output_file, index=False)
                        
                        result['status'] = 'completed'
                        result['records'] = len(df)
                        result['file'] = output_file.name
                        result['source_date'] = datetime.now().strftime('%Y-%m')
                        
                        self.logger.info(f"‚úì Downloaded {result['records']} EnviroStor sites for {county_name}")
                    else:
                        result['status'] = 'no_data'
                        self.logger.warning(f"No EnviroStor sites found for {county_name}")
                else:
                    result['status'] = 'no_data'
                    self.logger.warning(f"No data returned from EnviroStor API for {county_name}")
                    
        except Exception as e:
            self.logger.error(f"Failed to download EnviroStor data: {str(e)}")
            result['error'] = str(e)
            
        return result
        
    def download_geotracker_data(self, county_name: str, county_path: Path) -> Dict:
        """Download GeoTracker LUST/Cleanup sites"""
        result = {
            'source': 'CA SWRCB GeoTracker',
            'status': 'failed',
            'records': 0,
            'files': [],
            'url': 'https://geotracker.waterboards.ca.gov'
        }
        
        try:
            # GeoTracker provides statewide CSV files
            base_url = "https://geotracker.waterboards.ca.gov/data_download/"
            
            datasets = {
                'lust': 'geo_report.asp?global_id=&reporttype=1',  # LUST sites
                'cleanup': 'geo_report.asp?global_id=&reporttype=3'  # Cleanup Program sites
            }
            
            total_records = 0
            
            for dataset_type, endpoint in datasets.items():
                try:
                    url = base_url + endpoint
                    
                    # GeoTracker returns CSV directly
                    response = requests.get(url, timeout=120)
                    
                    if response.status_code == 200 and len(response.content) > 100:
                        # Parse CSV
                        df = pd.read_csv(StringIO(response.text), low_memory=False)
                        
                        # Filter for county
                        if 'COUNTY' in df.columns:
                            county_df = df[df['COUNTY'].str.contains(county_name, case=False, na=False)]
                            
                            if len(county_df) > 0:
                                output_file = county_path / f"{county_name.replace(' ', '_')}_geotracker_{dataset_type}.csv"
                                county_df.to_csv(output_file, index=False)
                                
                                result['files'].append(output_file.name)
                                total_records += len(county_df)
                                
                                self.logger.info(f"‚úì Downloaded {len(county_df)} GeoTracker {dataset_type} sites")
                                
                except Exception as e:
                    self.logger.warning(f"Could not download GeoTracker {dataset_type}: {str(e)}")
                    
            if total_records > 0:
                result['status'] = 'completed'
                result['records'] = total_records
                result['source_date'] = datetime.now().strftime('%Y-%m')
            else:
                result['status'] = 'no_data'
                
        except Exception as e:
            self.logger.error(f"Failed to download GeoTracker data: {str(e)}")
            result['error'] = str(e)
            
        return result
        
    def download_fema_flood_data(self, county_name: str, county_fips: str, county_path: Path) -> Dict:
        """Download FEMA flood zone data"""
        result = {
            'source': 'FEMA National Flood Hazard Layer',
            'status': 'failed',
            'records': 0,
            'file': None,
            'url': 'https://www.fema.gov/flood-maps/national-flood-hazard-layer'
        }
        
        try:
            # FEMA Map Service Center API
            # Use the NFHL layer (layer 28 is flood hazard areas)
            base_url = "https://hazards.fema.gov/gis/nfhl/rest/services/public/NFHL/MapServer"
            
            # Try multiple layer IDs as FEMA sometimes changes them
            layer_ids = [28, 3, 4]  # Flood hazard areas, FIRM panels, etc.
            
            for layer_id in layer_ids:
                try:
                    endpoint = f"{base_url}/{layer_id}/query"
                    
                    # Query by county FIPS code
                    params = {
                        'where': f"DFIRM_ID LIKE '{county_fips}%' OR STATE_FIPS = '06'",
                        'outFields': '*',
                        'f': 'json',
                        'returnGeometry': 'true',
                        'outSR': '4326',
                        'resultRecordCount': 5000
                    }
                    
                    response = requests.get(endpoint, params=params, timeout=60)
                    
                    if response.status_code == 200:
                        data = response.json()
                        
                        if 'features' in data and len(data['features']) > 0:
                            # Save as GeoJSON
                            geojson = {
                                'type': 'FeatureCollection',
                                'features': []
                            }
                            
                            for feature in data['features']:
                                geojson['features'].append({
                                    'type': 'Feature',
                                    'properties': feature.get('attributes', {}),
                                    'geometry': feature.get('geometry', {})
                                })
                                
                            output_file = county_path / f"{county_name.replace(' ', '_')}_fema_flood.geojson"
                            with open(output_file, 'w') as f:
                                json.dump(geojson, f)
                                
                            result['status'] = 'completed'
                            result['records'] = len(geojson['features'])
                            result['file'] = output_file.name
                            result['source_date'] = 'Current FEMA NFHL'
                            
                            self.logger.info(f"‚úì Downloaded {result['records']} FEMA flood zones for {county_name}")
                            break
                            
                except Exception as e:
                    self.logger.debug(f"Layer {layer_id} failed: {str(e)}")
                    continue
                    
            if result['status'] != 'completed':
                result['status'] = 'no_data'
                self.logger.warning(f"Could not download FEMA data for {county_name}")
                
        except Exception as e:
            self.logger.error(f"Failed to download FEMA data: {str(e)}")
            result['error'] = str(e)
            
        return result
        
    def process_county(self, county_name: str, county_fips: str):
        """Process all environmental data for a county"""
        self.logger.info(f"{'='*60}")
        self.logger.info(f"Processing {county_name} County (FIPS: {county_fips})")
        
        # Create county directory
        county_path = self.data_path / county_name.replace(' ', '_')
        county_path.mkdir(exist_ok=True)
        
        datasets = {}
        
        # Download each data source
        self.logger.info("Downloading EnviroStor data...")
        datasets['envirostor'] = self.download_envirostor_data(county_name, county_path)
        
        self.logger.info("Downloading GeoTracker data...")
        datasets['geotracker'] = self.download_geotracker_data(county_name, county_path)
        
        self.logger.info("Downloading FEMA flood data...")
        datasets['fema_flood'] = self.download_fema_flood_data(county_name, county_fips, county_path)
        
        # Create README.txt
        self.create_readme(county_path, county_name, datasets)
        
        # Summary
        total_records = sum(d.get('records', 0) for d in datasets.values())
        self.logger.info(f"‚úÖ Completed {county_name}: {total_records} total records")
        
        return datasets
        
def main():
    """Main execution"""
    print("California Environmental Data Downloader - WINGMAN EXECUTION")
    print("="*60)
    
    downloader = CaliforniaEnvironmentalDownloader()
    
    # Tier 1 counties
    counties = {
        'Los Angeles': '06037',
        'San Diego': '06073',
        'Orange': '06059',
        'San Francisco': '06075',
        'Alameda': '06001'
    }
    
    print(f"\nProcessing {len(counties)} Tier 1 counties...")
    
    results = {}
    for county, fips in counties.items():
        results[county] = downloader.process_county(county, fips)
        
    print("\n‚úÖ DOWNLOAD COMPLETE")
    print(f"üìÅ Output: data_sets/california/CA_Environmental_Data/")
    
    # Summary report
    print("\nSUMMARY:")
    for county, datasets in results.items():
        total = sum(d.get('records', 0) for d in datasets.values())
        print(f"  {county}: {total} records")

if __name__ == "__main__":
    main()