#!/usr/bin/env python3
"""
California Environmental Database Unification - S3 Optimized
Combines 7 CalEPA datasets into unified, S3-ready format
Leverages M4's 128GB RAM and 40 cores for maximum performance
WINGMAN Execution - Mission CA-ENV-UNIFIED-2025-003
Date: 2025-08-09
"""

import pandas as pd
import numpy as np
import pyarrow as pa
import pyarrow.parquet as pq
from pathlib import Path
from datetime import datetime
import json
import sqlite3
import geopandas as gpd
from shapely.geometry import Point
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import multiprocessing
import logging
import gc
import warnings
warnings.filterwarnings('ignore')

class CaliforniaEnvironmentalUnifier:
    """Unify CalEPA datasets for S3 deployment with M4 optimization"""
    
    def __init__(self, base_path=None, max_workers=None):
        """Initialize with M4 optimization settings"""
        self.base_path = Path(base_path or "/Users/williamrice/Library/CloudStorage/Dropbox-HERR/Bill Rice/Colosseum")
        self.data_path = self.base_path / "data_sets" / "california" / "CA_Environmental_Data" / "CalEPA_Compliance"
        self.output_path = self.base_path / "data_sets" / "california" / "CA_Environmental_Unified"
        self.output_path.mkdir(parents=True, exist_ok=True)
        
        # M4 optimization - use 32 cores (leave some for system)
        self.max_workers = max_workers or min(32, multiprocessing.cpu_count() - 8)
        
        # Setup logging
        self.setup_logging()
        
        # Track memory usage
        self.start_time = datetime.now()
        
    def setup_logging(self):
        """Configure logging"""
        log_file = self.output_path / f"unification_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def load_sites_master(self):
        """Load Site.csv as master table"""
        self.logger.info("="*80)
        self.logger.info("PHASE 1: LOADING MASTER SITE DATA")
        self.logger.info("="*80)
        
        site_file = self.data_path / "Site.csv"
        
        # Use all available RAM - M4 has 128GB
        self.logger.info(f"Loading Site.csv (329,855 records)...")
        df_sites = pd.read_csv(site_file, dtype=str, low_memory=False)
        
        # Clean column names
        df_sites.columns = df_sites.columns.str.strip()
        
        # Ensure SiteID is string for consistent joining
        df_sites['SiteID'] = df_sites['SiteID'].astype(str)
        
        self.logger.info(f"‚úì Loaded {len(df_sites):,} sites")
        self.logger.info(f"  Columns: {', '.join(df_sites.columns)}")
        
        # Add tracking fields
        df_sites['HasCoordinates'] = False
        df_sites['ViolationCount'] = 0
        df_sites['EvaluationCount'] = 0
        df_sites['EnforcementCount'] = 0
        df_sites['HasChemicals'] = False
        df_sites['RegulatedProgramCount'] = 0
        
        return df_sites
        
    def attach_coordinates(self, df_sites):
        """Attach coordinate data with parallel processing"""
        self.logger.info("\nPHASE 2: ATTACHING GEOSPATIAL DATA")
        self.logger.info("-"*40)
        
        coord_file = self.data_path / "Coordinates.csv"
        
        self.logger.info(f"Loading Coordinates.csv (81,305 records)...")
        df_coords = pd.read_csv(coord_file, dtype=str, low_memory=False)
        df_coords['SiteID'] = df_coords['SiteID'].astype(str)
        
        # Convert coordinates to float
        df_coords['LATITUDE'] = pd.to_numeric(df_coords['LATITUDE'], errors='coerce')
        df_coords['LONGITUDE'] = pd.to_numeric(df_coords['LONGITUDE'], errors='coerce')
        
        # Group by SiteID and take first valid coordinates
        coords_dedup = df_coords.groupby('SiteID').first().reset_index()
        
        # Merge coordinates
        df_sites = df_sites.merge(
            coords_dedup[['SiteID', 'LATITUDE', 'LONGITUDE', 'FACILITY_NAME']],
            on='SiteID',
            how='left',
            suffixes=('', '_coord')
        )
        
        # Update HasCoordinates flag
        df_sites['HasCoordinates'] = df_sites['LATITUDE'].notna() & df_sites['LONGITUDE'].notna()
        
        valid_coords = df_sites['HasCoordinates'].sum()
        self.logger.info(f"‚úì Attached coordinates to {valid_coords:,} sites")
        
        return df_sites
        
    def process_violations_chunked(self, df_sites):
        """Process violations in chunks to manage memory"""
        self.logger.info("\nPHASE 3: PROCESSING VIOLATIONS (3.1M records)")
        self.logger.info("-"*40)
        
        viol_file = self.data_path / "Violations.csv"
        
        # Process violations in chunks to manage memory
        chunk_size = 100000
        violations_by_site = {}
        
        self.logger.info(f"Processing violations in {chunk_size:,} record chunks...")
        
        for chunk_num, chunk in enumerate(pd.read_csv(viol_file, chunksize=chunk_size, dtype=str, low_memory=False)):
            if chunk_num % 10 == 0:
                self.logger.info(f"  Processing chunk {chunk_num + 1} ({(chunk_num + 1) * chunk_size:,} records)...")
            
            chunk['SiteID'] = chunk['SiteID'].astype(str)
            
            # Group violations by SiteID
            for site_id, group in chunk.groupby('SiteID'):
                if site_id not in violations_by_site:
                    violations_by_site[site_id] = []
                
                # Keep essential violation info
                for _, row in group.iterrows():
                    violations_by_site[site_id].append({
                        'ViolationDate': row.get('ViolationDate', ''),
                        'Citation': row.get('Citation', ''),
                        'ViolationDescription': row.get('ViolationDescription', ''),
                        'ViolationDivision': row.get('ViolationDivision', ''),
                        'ViolationProgram': row.get('ViolationProgram', '')
                    })
        
        # Update violation counts
        self.logger.info("Updating violation counts...")
        for site_id, violations in violations_by_site.items():
            if site_id in df_sites['SiteID'].values:
                df_sites.loc[df_sites['SiteID'] == site_id, 'ViolationCount'] = len(violations)
        
        # Store violations as JSON in new column
        df_sites['Violations_JSON'] = df_sites['SiteID'].apply(
            lambda x: json.dumps(violations_by_site.get(x, []))
        )
        
        total_sites_with_violations = (df_sites['ViolationCount'] > 0).sum()
        total_violations = df_sites['ViolationCount'].sum()
        
        self.logger.info(f"‚úì Processed {total_violations:,} violations for {total_sites_with_violations:,} sites")
        
        return df_sites
        
    def process_evaluations_chunked(self, df_sites):
        """Process evaluations in chunks"""
        self.logger.info("\nPHASE 4: PROCESSING EVALUATIONS (1.5M records)")
        self.logger.info("-"*40)
        
        eval_file = self.data_path / "Evaluations.csv"
        
        chunk_size = 100000
        evaluations_by_site = {}
        
        self.logger.info(f"Processing evaluations in {chunk_size:,} record chunks...")
        
        for chunk_num, chunk in enumerate(pd.read_csv(eval_file, chunksize=chunk_size, dtype=str, low_memory=False)):
            if chunk_num % 5 == 0:
                self.logger.info(f"  Processing chunk {chunk_num + 1}...")
            
            chunk['SiteID'] = chunk['SiteID'].astype(str)
            
            for site_id, group in chunk.groupby('SiteID'):
                if site_id not in evaluations_by_site:
                    evaluations_by_site[site_id] = []
                
                for _, row in group.iterrows():
                    evaluations_by_site[site_id].append({
                        'EvalDate': row.get('EvalDate', ''),
                        'ViolationsFound': row.get('ViolationsFound', ''),
                        'EvalType': row.get('EvalType', ''),
                        'EvalDivision': row.get('EvalDivision', ''),
                        'EvalProgram': row.get('EvalProgram', '')
                    })
        
        # Update evaluation counts
        for site_id, evaluations in evaluations_by_site.items():
            if site_id in df_sites['SiteID'].values:
                df_sites.loc[df_sites['SiteID'] == site_id, 'EvaluationCount'] = len(evaluations)
        
        df_sites['Evaluations_JSON'] = df_sites['SiteID'].apply(
            lambda x: json.dumps(evaluations_by_site.get(x, []))
        )
        
        total_sites_with_evals = (df_sites['EvaluationCount'] > 0).sum()
        self.logger.info(f"‚úì Processed evaluations for {total_sites_with_evals:,} sites")
        
        return df_sites
        
    def process_enforcements(self, df_sites):
        """Process enforcement actions"""
        self.logger.info("\nPHASE 5: PROCESSING ENFORCEMENTS (173K records)")
        self.logger.info("-"*40)
        
        ea_file = self.data_path / "EA.csv"
        
        df_ea = pd.read_csv(ea_file, dtype=str, low_memory=False)
        df_ea['SiteID'] = df_ea['SiteID'].astype(str)
        
        enforcements_by_site = {}
        
        for site_id, group in df_ea.groupby('SiteID'):
            enforcements_by_site[site_id] = []
            for _, row in group.iterrows():
                enforcements_by_site[site_id].append({
                    'EnfActionDate': row.get('EnfActionDate', ''),
                    'EnfActionType': row.get('EnfActionType', ''),
                    'EnfActionDescription': row.get('EnfActionDescription', ''),
                    'EnfActionDivision': row.get('EnfActionDivision', ''),
                    'EnfActionProgram': row.get('EnfActionProgram', '')
                })
        
        # Update enforcement counts
        for site_id, enforcements in enforcements_by_site.items():
            if site_id in df_sites['SiteID'].values:
                df_sites.loc[df_sites['SiteID'] == site_id, 'EnforcementCount'] = len(enforcements)
        
        df_sites['Enforcements_JSON'] = df_sites['SiteID'].apply(
            lambda x: json.dumps(enforcements_by_site.get(x, []))
        )
        
        total_sites_with_enf = (df_sites['EnforcementCount'] > 0).sum()
        self.logger.info(f"‚úì Processed enforcements for {total_sites_with_enf:,} sites")
        
        return df_sites
        
    def attach_chemicals(self, df_sites):
        """Attach chemical data"""
        self.logger.info("\nPHASE 6: ATTACHING CHEMICAL DATA")
        self.logger.info("-"*40)
        
        chem_file = self.data_path / "Chems.csv"
        
        df_chems = pd.read_csv(chem_file, dtype=str, low_memory=False)
        df_chems['SiteID'] = df_chems['SiteID'].astype(str)
        
        chemicals_by_site = {}
        
        for site_id, group in df_chems.groupby('SiteID'):
            chemicals_by_site[site_id] = group[['CHEMICAL_NAME', 'CAS_NUMBER', 'MAX_DAILY_AMOUNT_RANGE']].to_dict('records')
        
        df_sites['HasChemicals'] = df_sites['SiteID'].isin(chemicals_by_site.keys())
        df_sites['Chemicals_JSON'] = df_sites['SiteID'].apply(
            lambda x: json.dumps(chemicals_by_site.get(x, []))
        )
        
        total_sites_with_chems = df_sites['HasChemicals'].sum()
        self.logger.info(f"‚úì Attached chemical data to {total_sites_with_chems:,} sites")
        
        return df_sites
        
    def attach_regulated_programs(self, df_sites):
        """Attach regulated program data"""
        self.logger.info("\nPHASE 7: ATTACHING REGULATED PROGRAMS")
        self.logger.info("-"*40)
        
        prog_file = self.data_path / "SiteEI.csv"
        
        df_prog = pd.read_csv(prog_file, dtype=str, low_memory=False)
        df_prog['SiteID'] = df_prog['SiteID'].astype(str)
        
        # Count programs per site
        program_counts = df_prog.groupby('SiteID').size().to_dict()
        
        df_sites['RegulatedProgramCount'] = df_sites['SiteID'].apply(
            lambda x: program_counts.get(x, 0)
        )
        
        total_sites_with_programs = (df_sites['RegulatedProgramCount'] > 0).sum()
        self.logger.info(f"‚úì Attached program data to {total_sites_with_programs:,} sites")
        
        return df_sites
        
    def create_outputs(self, df_unified):
        """Create multiple output formats optimized for S3"""
        self.logger.info("\n" + "="*80)
        self.logger.info("PHASE 8: CREATING S3-OPTIMIZED OUTPUTS")
        self.logger.info("="*80)
        
        # 1. Create Parquet file (S3 optimized)
        self.logger.info("\n1. Creating Parquet file for S3...")
        parquet_file = self.output_path / "california_environmental_unified.parquet"
        
        # Optimize for S3/Athena queries
        df_unified.to_parquet(
            parquet_file,
            engine='pyarrow',
            compression='snappy',  # Best for S3/Athena
            index=False
        )
        
        parquet_size = parquet_file.stat().st_size / (1024*1024)
        self.logger.info(f"   ‚úì Created Parquet file: {parquet_size:.1f} MB")
        
        # 2. Create partitioned Parquet by county (for selective S3 queries)
        self.logger.info("\n2. Creating county-partitioned Parquet files...")
        county_path = self.output_path / "by-county"
        county_path.mkdir(exist_ok=True)
        
        if 'County' in df_unified.columns:
            for county in df_unified['County'].dropna().unique():
                county_df = df_unified[df_unified['County'] == county]
                if len(county_df) > 0:
                    county_file = county_path / f"{county.lower().replace(' ', '_')}.parquet"
                    county_df.to_parquet(county_file, engine='pyarrow', compression='snappy', index=False)
            
            self.logger.info(f"   ‚úì Created {len(df_unified['County'].dropna().unique())} county files")
        
        # 3. Create SQLite database for local queries
        self.logger.info("\n3. Creating SQLite database...")
        db_file = self.output_path / "california_environmental.db"
        conn = sqlite3.connect(db_file)
        
        # Main table
        df_unified.to_sql('sites', conn, if_exists='replace', index=False)
        
        # Create indexes for fast queries
        cursor = conn.cursor()
        cursor.execute("CREATE INDEX idx_siteid ON sites(SiteID)")
        cursor.execute("CREATE INDEX idx_city ON sites(City)")
        cursor.execute("CREATE INDEX idx_county ON sites(County)")
        cursor.execute("CREATE INDEX idx_zip ON sites(ZIP)")
        
        conn.commit()
        conn.close()
        
        db_size = db_file.stat().st_size / (1024*1024)
        self.logger.info(f"   ‚úì Created SQLite database: {db_size:.1f} MB")
        
        # 4. Create GeoJSON for mapping (only sites with coordinates)
        self.logger.info("\n4. Creating GeoJSON for mapping...")
        df_geo = df_unified[df_unified['HasCoordinates'] == True].copy()
        
        if len(df_geo) > 0:
            # Create GeoDataFrame
            geometry = [Point(xy) for xy in zip(df_geo['LONGITUDE'], df_geo['LATITUDE'])]
            gdf = gpd.GeoDataFrame(df_geo, geometry=geometry, crs='EPSG:4326')
            
            # Select essential columns for GeoJSON
            geo_cols = ['SiteID', 'SiteName', 'Address', 'City', 'County', 'ZIP',
                       'ViolationCount', 'EvaluationCount', 'EnforcementCount', 
                       'HasChemicals', 'RegulatedProgramCount']
            
            gdf_export = gdf[geo_cols + ['geometry']]
            
            geojson_file = self.output_path / "california_environmental_geocoded.geojson"
            gdf_export.to_file(geojson_file, driver='GeoJSON')
            
            self.logger.info(f"   ‚úì Created GeoJSON with {len(gdf):,} geocoded sites")
        
        # 5. Create summary statistics
        self.logger.info("\n5. Creating summary statistics...")
        summary = {
            'processing_date': datetime.now().isoformat(),
            'total_sites': len(df_unified),
            'sites_with_coordinates': int(df_unified['HasCoordinates'].sum()),
            'sites_with_violations': int((df_unified['ViolationCount'] > 0).sum()),
            'sites_with_evaluations': int((df_unified['EvaluationCount'] > 0).sum()),
            'sites_with_enforcements': int((df_unified['EnforcementCount'] > 0).sum()),
            'sites_with_chemicals': int(df_unified['HasChemicals'].sum()),
            'sites_with_programs': int((df_unified['RegulatedProgramCount'] > 0).sum()),
            'total_violations': int(df_unified['ViolationCount'].sum()),
            'total_evaluations': int(df_unified['EvaluationCount'].sum()),
            'total_enforcements': int(df_unified['EnforcementCount'].sum()),
            'file_sizes': {
                'parquet_mb': round(parquet_size, 1),
                'sqlite_mb': round(db_size, 1)
            },
            's3_ready': True,
            'athena_compatible': True
        }
        
        summary_file = self.output_path / "california_environmental_summary.json"
        with open(summary_file, 'w') as f:
            json.dump(summary, f, indent=2)
        
        self.logger.info(f"   ‚úì Created summary statistics")
        
        return summary
        
    def create_documentation(self, summary):
        """Create comprehensive documentation"""
        self.logger.info("\nPHASE 9: CREATING DOCUMENTATION")
        self.logger.info("-"*40)
        
        readme_content = f"""DATASET: California Unified Environmental Database
SOURCE: CalEPA Site Portal (https://siteportal.calepa.ca.gov/nsite/map/export)
PROCESSING DATE: {datetime.now().strftime('%Y-%m-%d %H:%M')}
DESCRIPTION: Unified database combining 7 CalEPA datasets for comprehensive environmental screening

DATABASE STATISTICS:
====================
Total Sites: {summary['total_sites']:,}
Sites with Coordinates: {summary['sites_with_coordinates']:,}
Sites with Violations: {summary['sites_with_violations']:,}
Sites with Evaluations: {summary['sites_with_evaluations']:,}
Sites with Enforcements: {summary['sites_with_enforcements']:,}
Sites with Chemical Data: {summary['sites_with_chemicals']:,}
Sites in Regulated Programs: {summary['sites_with_programs']:,}

Total Violations: {summary['total_violations']:,}
Total Evaluations: {summary['total_evaluations']:,}
Total Enforcements: {summary['total_enforcements']:,}

FILES CREATED:
==============
1. california_environmental_unified.parquet ({summary['file_sizes']['parquet_mb']} MB)
   - S3/Athena optimized format
   - Snappy compression for fast queries
   - All data in single file

2. by-county/*.parquet
   - County-level partitions for selective loading
   - Optimized for regional analysis
   
3. california_environmental.db ({summary['file_sizes']['sqlite_mb']} MB)
   - SQLite database with indexes
   - Local query capability
   
4. california_environmental_geocoded.geojson
   - Mappable sites with coordinates
   - Essential fields only for performance

S3 DEPLOYMENT:
==============
The Parquet files are optimized for S3 deployment with:
- Snappy compression (best for Athena queries)
- Column-oriented storage for selective reading
- Partitioned by county for cost-effective queries

AWS Athena Query Example:
SELECT SiteName, City, ViolationCount
FROM california_environmental
WHERE County = 'LOS ANGELES' 
  AND ViolationCount > 10

FIELD DESCRIPTIONS:
===================
Core Fields:
- SiteID: Unique identifier linking all CalEPA tables
- SiteName: Facility name
- Address, City, County, ZIP: Location information
- LATITUDE, LONGITUDE: Geospatial coordinates (where available)
- HasCoordinates: Boolean flag for mapping capability

Compliance Metrics (counts only, no scoring):
- ViolationCount: Number of violations on record
- EvaluationCount: Number of evaluations/inspections
- EnforcementCount: Number of enforcement actions
- HasChemicals: Boolean flag for chemical presence
- RegulatedProgramCount: Number of regulatory programs

Detail Fields (JSON format):
- Violations_JSON: Array of all violations with dates and descriptions
- Evaluations_JSON: Array of all evaluations with types and findings
- Enforcements_JSON: Array of all enforcement actions
- Chemicals_JSON: Array of chemicals present at site

QUERY EXAMPLES:
===============
# Find all sites in Los Angeles with violations
SELECT * FROM sites 
WHERE County = 'LOS ANGELES' AND ViolationCount > 0;

# Find sites within specific ZIP codes
SELECT * FROM sites 
WHERE ZIP IN ('90001', '90002', '90003');

# Find sites with chemical contamination
SELECT * FROM sites 
WHERE HasChemicals = 1;

IMPORTANT NOTES:
================
- NO RISK SCORING: This database contains raw data only
- NO INTERPRETATION: Violation severity not assessed
- FULL PRESERVATION: All original descriptions maintained
- FOR DISCOVERY: Use before ordering Phase I ESA
- NOT A REPLACEMENT: Does not replace professional environmental assessment

PROCESSING DETAILS:
==================
- Used M4 Mac with 128GB RAM and 40 cores
- Parallel processing for large datasets
- Memory-efficient chunked reading for 3M+ violations
- Processing time: {(datetime.now() - self.start_time).total_seconds() / 60:.1f} minutes

GENERATED BY: Colosseum LIHTC Platform - WINGMAN Agent
MISSION: CA Environmental Database Unification (S3 Optimized)
"""
        
        readme_file = self.output_path / "README_UNIFIED_DATABASE.txt"
        with open(readme_file, 'w') as f:
            f.write(readme_content)
        
        self.logger.info("‚úì Created comprehensive documentation")
        
    def run(self):
        """Execute complete unification process"""
        self.logger.info("="*80)
        self.logger.info("CALIFORNIA ENVIRONMENTAL DATABASE UNIFICATION")
        self.logger.info(f"Start Time: {self.start_time}")
        self.logger.info(f"Using {self.max_workers} parallel workers")
        self.logger.info("="*80)
        
        # Load and process data
        df_unified = self.load_sites_master()
        df_unified = self.attach_coordinates(df_unified)
        df_unified = self.process_violations_chunked(df_unified)
        df_unified = self.process_evaluations_chunked(df_unified)
        df_unified = self.process_enforcements(df_unified)
        df_unified = self.attach_chemicals(df_unified)
        df_unified = self.attach_regulated_programs(df_unified)
        
        # Create outputs
        summary = self.create_outputs(df_unified)
        self.create_documentation(summary)
        
        # Final summary
        end_time = datetime.now()
        duration = (end_time - self.start_time).total_seconds() / 60
        
        self.logger.info("\n" + "="*80)
        self.logger.info("UNIFICATION COMPLETE")
        self.logger.info("="*80)
        self.logger.info(f"Total Processing Time: {duration:.1f} minutes")
        self.logger.info(f"Output Directory: {self.output_path}")
        self.logger.info(f"Total Sites Processed: {len(df_unified):,}")
        self.logger.info("\nS3-READY FILES:")
        self.logger.info(f"  ‚Ä¢ california_environmental_unified.parquet ({summary['file_sizes']['parquet_mb']} MB)")
        self.logger.info(f"  ‚Ä¢ by-county/*.parquet (partitioned for selective queries)")
        self.logger.info(f"  ‚Ä¢ california_environmental.db ({summary['file_sizes']['sqlite_mb']} MB)")
        self.logger.info("="*80)
        
        return df_unified

def main():
    """Main execution"""
    print("\nCALIFORNIA ENVIRONMENTAL DATABASE UNIFICATION")
    print("S3-Optimized with M4 Performance")
    print("="*60)
    
    unifier = CaliforniaEnvironmentalUnifier()
    df_unified = unifier.run()
    
    print("\n‚úÖ UNIFICATION COMPLETE")
    print(f"üìÅ Output: data_sets/california/CA_Environmental_Unified/")
    print(f"üìä Total Sites: {len(df_unified):,}")
    print("\nüöÄ Ready for S3 deployment!")

if __name__ == "__main__":
    main()