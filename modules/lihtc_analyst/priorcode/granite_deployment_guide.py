#!/usr/bin/env python3
"""
IBM Granite Model Deployment Guide for M4 Beast
Integrate Granite models for LIHTC/QAP document extraction
"""

import subprocess
import json
import requests
from pathlib import Path
from typing import Dict, List, Optional
import time

class GraniteDeploymentManager:
    """Manage IBM Granite model deployment and integration"""
    
    def __init__(self):
        self.available_granite_models = {
            # IBM Granite 3.1 series (latest)
            'granite-3.1-2b-instruct': {
                'size': '2B parameters',
                'use_case': 'Fast inference, address extraction',
                'huggingface_id': 'ibm-granite/granite-3.1-2b-instruct',
                'memory_req': '4GB'
            },
            'granite-3.1-8b-instruct': {
                'size': '8B parameters', 
                'use_case': 'Balanced performance, regulatory text',
                'huggingface_id': 'ibm-granite/granite-3.1-8b-instruct',
                'memory_req': '16GB'
            },
            'granite-3.1-34b-instruct': {
                'size': '34B parameters',
                'use_case': 'Enterprise-grade, complex QAP analysis',
                'huggingface_id': 'ibm-granite/granite-3.1-34b-instruct', 
                'memory_req': '64GB'
            },
            # Code-specific models
            'granite-3.0-8b-code-instruct': {
                'size': '8B parameters',
                'use_case': 'Structured data extraction',
                'huggingface_id': 'ibm-granite/granite-3.0-8b-code-instruct',
                'memory_req': '16GB'
            }
        }
        
        # Granite model strengths for LIHTC processing
        self.granite_advantages = {
            'enterprise_focus': 'Designed for business document processing',
            'regulatory_understanding': 'Trained on compliance and regulatory text',
            'structured_extraction': 'Excellent at parsing tables and forms',
            'open_source': 'Apache 2.0 license - fully deployable locally',
            'docling_integration': 'Official IBM Docling compatibility'
        }
    
    def check_system_requirements(self) -> Dict[str, bool]:
        """Check if M4 Beast can handle Granite models"""
        
        print("ğŸ” Checking M4 Beast System Requirements")
        print("-" * 50)
        
        requirements = {
            'ollama_running': self._check_ollama(),
            'sufficient_memory': self._check_memory(),
            'huggingface_cli': self._check_huggingface_cli(),
            'python_env': self._check_python_env()
        }
        
        for req, status in requirements.items():
            status_icon = "âœ…" if status else "âŒ"
            print(f"{status_icon} {req.replace('_', ' ').title()}: {'Ready' if status else 'Needs Setup'}")
        
        return requirements
    
    def _check_ollama(self) -> bool:
        """Check if Ollama is running"""
        try:
            result = subprocess.run(['ollama', 'list'], capture_output=True, text=True, timeout=10)
            return result.returncode == 0
        except:
            return False
    
    def _check_memory(self) -> bool:
        """Check available memory (simplified)"""
        # On M4 Beast, should have sufficient memory for 34B models
        return True  # Assume M4 Beast has adequate RAM
    
    def _check_huggingface_cli(self) -> bool:
        """Check if Hugging Face CLI is available"""
        try:
            result = subprocess.run(['huggingface-cli', '--version'], capture_output=True, text=True, timeout=5)
            return result.returncode == 0
        except:
            return False
    
    def _check_python_env(self) -> bool:
        """Check Python environment"""
        try:
            import transformers
            import torch
            return True
        except ImportError:
            return False
    
    def install_granite_via_ollama(self, model_name: str = 'granite-3.1-8b-instruct') -> bool:
        """Install Granite model via Ollama (if available)"""
        
        print(f"\nğŸš€ Attempting to install {model_name} via Ollama")
        
        # Try to pull from Ollama registry
        ollama_model_names = [
            f"granite:{model_name}",
            f"ibm/granite:{model_name.split('-')[-1]}",
            f"granite-{model_name.split('-')[1]}"
        ]
        
        for ollama_name in ollama_model_names:
            try:
                print(f"  Trying: ollama pull {ollama_name}")
                result = subprocess.run(
                    ['ollama', 'pull', ollama_name], 
                    capture_output=True, 
                    text=True, 
                    timeout=300  # 5 minutes max
                )
                
                if result.returncode == 0:
                    print(f"âœ… Successfully installed {ollama_name}")
                    return True
                else:
                    print(f"  âŒ Failed: {result.stderr}")
                    
            except subprocess.TimeoutExpired:
                print(f"  â° Timeout pulling {ollama_name}")
            except Exception as e:
                print(f"  âŒ Error: {e}")
        
        print("âŒ Ollama installation failed - trying alternative methods")
        return False
    
    def install_granite_via_huggingface(self, model_name: str = 'granite-3.1-8b-instruct') -> bool:
        """Install Granite model via Hugging Face"""
        
        model_info = self.available_granite_models.get(model_name)
        if not model_info:
            print(f"âŒ Unknown model: {model_name}")
            return False
        
        print(f"\nğŸ“¥ Installing {model_name} via Hugging Face")
        print(f"Model: {model_info['huggingface_id']}")
        print(f"Memory Required: {model_info['memory_req']}")
        
        try:
            # Download model using transformers
            print("Downloading model files...")
            
            # Create installation script
            install_script = f"""
import os
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

print("ğŸ”¥ Loading {model_name} from Hugging Face...")

# Set cache directory
cache_dir = os.path.expanduser("~/.cache/huggingface/transformers")
os.makedirs(cache_dir, exist_ok=True)

try:
    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        "{model_info['huggingface_id']}",
        cache_dir=cache_dir
    )
    
    # Load model with appropriate precision for M4 Beast
    model = AutoModelForCausalLM.from_pretrained(
        "{model_info['huggingface_id']}",
        torch_dtype=torch.float16,  # Use half precision for memory efficiency
        device_map="auto",  # Auto-assign to available devices
        cache_dir=cache_dir
    )
    
    print("âœ… Model loaded successfully!")
    print(f"Model device: {{model.device}}")
    print(f"Model dtype: {{model.dtype}}")
    
    # Save model info
    model_info = {{
        "model_name": "{model_name}",
        "huggingface_id": "{model_info['huggingface_id']}",
        "loaded": True,
        "device": str(model.device),
        "memory_footprint": "{model_info['memory_req']}"
    }}
    
    with open("granite_model_info.json", "w") as f:
        import json
        json.dump(model_info, f, indent=2)
    
    print("ğŸ’¾ Model info saved to granite_model_info.json")
    
except Exception as e:
    print(f"âŒ Failed to load model: {{e}}")
    exit(1)
"""
            
            # Write and execute installation script
            script_path = Path(__file__).parent / "install_granite.py"
            with open(script_path, 'w') as f:
                f.write(install_script)
            
            print(f"ğŸ“ Installation script created: {script_path}")
            print("ğŸš€ Run this to install Granite model:")
            print(f"cd {Path(__file__).parent}")
            print("python3 install_granite.py")
            
            return True
            
        except Exception as e:
            print(f"âŒ Error creating installation: {e}")
            return False
    
    def create_granite_benchmark_test(self) -> str:
        """Create benchmark test specifically for Granite vs current system"""
        
        test_script = '''#!/usr/bin/env python3
"""
Granite Model Benchmark for LIHTC Address Extraction
Test Granite against our failing address cases
"""

import json
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import time

class GraniteLIHTCBenchmark:
    
    def __init__(self, model_path="ibm-granite/granite-3.1-8b-instruct"):
        print(f"ğŸ”¥ Loading Granite model: {model_path}")
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        # Test cases from TDHCA benchmark (our failing cases)
        self.test_cases = [
            {
                "id": "tdhca_fail_001",
                "input": "Development Site: Located at 9012 North Elm Street, Suite 100 in the city of Austin, Travis County, Texas",
                "expected": "9012 North Elm Street, Suite 100",
                "difficulty": "hard"
            },
            {
                "id": "tdhca_fail_002", 
                "input": """Property Information:
Address Line 1: 3456 Technology Drive
Address Line 2: Building A
City: San Antonio
County: Bexar
State: Texas""",
                "expected": "3456 Technology Drive, Building A",
                "difficulty": "very_hard"
            },
            {
                "id": "tdhca_fail_003",
                "input": "Developer office located at 1111 Business Blvd. Project site address: 7890 Innovation Way, Fort Worth, TX.",
                "expected": "7890 Innovation Way",
                "difficulty": "hard"
            }
        ]
    
    def extract_address(self, text: str) -> str:
        """Extract address using Granite model"""
        
        prompt = f"""Extract the street address of the development project from this text. Return ONLY the street address.

Text: {text}

Street Address:"""

        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=50,
                temperature=0.1,
                do_sample=False,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Extract just the generated part
        generated = response[len(prompt):].strip()
        return generated
    
    def run_benchmark(self):
        """Run benchmark against failing TDHCA cases"""
        
        print("ğŸ¯ Granite LIHTC Address Extraction Benchmark")
        print("=" * 60)
        
        results = []
        
        for test_case in self.test_cases:
            print(f"\\nğŸ“ Test: {test_case['id']} ({test_case['difficulty']})")
            
            start_time = time.time()
            extracted = self.extract_address(test_case['input'])
            processing_time = time.time() - start_time
            
            # Simple validation
            expected_clean = test_case['expected'].lower().replace(',', '').replace('.', '')
            extracted_clean = extracted.lower().replace(',', '').replace('.', '')
            
            success = expected_clean in extracted_clean or extracted_clean in expected_clean
            
            result = {
                'test_id': test_case['id'],
                'expected': test_case['expected'],
                'extracted': extracted,
                'success': success,
                'processing_time': processing_time,
                'difficulty': test_case['difficulty']
            }
            
            results.append(result)
            
            status = "âœ…" if success else "âŒ"
            print(f"  {status} Expected: {test_case['expected']}")
            print(f"     Extracted: {extracted}")
            print(f"     Time: {processing_time:.2f}s")
        
        # Calculate summary
        success_count = sum(1 for r in results if r['success'])
        accuracy = (success_count / len(results)) * 100
        avg_time = sum(r['processing_time'] for r in results) / len(results)
        
        print(f"\\nğŸ“Š Granite Results Summary:")
        print(f"Accuracy: {accuracy:.1f}% ({success_count}/{len(results)})")
        print(f"Avg Time: {avg_time:.2f}s")
        print(f"vs Current System: {accuracy:.1f}% vs 26.9% (+{accuracy-26.9:.1f} points)")
        
        # Save results
        with open('granite_benchmark_results.json', 'w') as f:
            json.dump({
                'model': 'granite-3.1-8b-instruct',
                'accuracy': accuracy,
                'avg_processing_time': avg_time,
                'improvement_over_current': accuracy - 26.9,
                'detailed_results': results
            }, f, indent=2)
        
        print("ğŸ’¾ Results saved to granite_benchmark_results.json")
        
        return results

if __name__ == "__main__":
    try:
        benchmark = GraniteLIHTCBenchmark()
        benchmark.run_benchmark()
    except Exception as e:
        print(f"âŒ Benchmark failed: {e}")
        print("Make sure Granite model is installed first!")
'''
        
        # Save benchmark script
        script_path = Path(__file__).parent / "granite_lihtc_benchmark.py"
        with open(script_path, 'w') as f:
            f.write(test_script)
        
        return str(script_path)
    
    def recommend_deployment_strategy(self) -> Dict[str, str]:
        """Recommend best Granite deployment for M4 Beast"""
        
        print("\nğŸ’¡ Granite Deployment Recommendation for M4 Beast")
        print("=" * 60)
        
        strategy = {
            'primary_model': 'granite-3.1-8b-instruct',
            'reasoning': 'Balanced performance/memory for address extraction',
            'fallback_model': 'granite-3.1-2b-instruct', 
            'integration_approach': 'Hybrid with existing Llama system',
            'deployment_method': 'Hugging Face Transformers (local)',
            'expected_improvement': '+40-60 points over current 26.9% accuracy'
        }
        
        print(f"ğŸ¯ Recommended Model: {strategy['primary_model']}")
        print(f"ğŸ’­ Reasoning: {strategy['reasoning']}")
        print(f"ğŸ”„ Integration: {strategy['integration_approach']}")
        print(f"ğŸ“ˆ Expected Improvement: {strategy['expected_improvement']}")
        
        print("\nğŸ“‹ Implementation Steps:")
        print("1. Install granite-3.1-8b-instruct via Hugging Face")
        print("2. Run granite_lihtc_benchmark.py to validate")
        print("3. Integrate with hybrid_extraction_orchestrator.py")
        print("4. Compare Granite vs Llama on full TDHCA dataset")
        print("5. Deploy winning model for production QAP processing")
        
        return strategy

def main():
    """Main deployment workflow"""
    
    print("ğŸ—ï¸ IBM Granite Model Deployment for M4 Beast")
    print("Targeting LIHTC Address Extraction (26.9% â†’ 90%+ accuracy)")
    print("=" * 70)
    
    manager = GraniteDeploymentManager()
    
    # Check system readiness
    requirements = manager.check_system_requirements()
    
    if not all(requirements.values()):
        print("\nâš ï¸ System requirements not met. Please install missing components:")
        if not requirements['huggingface_cli']:
            print("  pip install huggingface_hub")
        if not requirements['python_env']:
            print("  pip install transformers torch")
    
    # Try Ollama first (faster), then Hugging Face
    model_installed = False
    
    if requirements['ollama_running']:
        print("\nğŸš€ Attempting Ollama installation...")
        model_installed = manager.install_granite_via_ollama()
    
    if not model_installed:
        print("\nğŸ“¥ Using Hugging Face installation...")
        model_installed = manager.install_granite_via_huggingface()
    
    # Create benchmark test
    benchmark_script = manager.create_granite_benchmark_test()
    print(f"\nğŸ§ª Benchmark test created: {benchmark_script}")
    
    # Provide deployment strategy
    strategy = manager.recommend_deployment_strategy()
    
    print("\nâœ… Granite deployment guide complete!")
    print("\nNext steps:")
    print("1. Run the installation script")
    print("2. Execute granite_lihtc_benchmark.py")
    print("3. Compare results with Llama 8B (currently 100% accuracy)")

if __name__ == "__main__":
    main()